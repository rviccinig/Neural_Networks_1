{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision with the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Reproducibility Purposes we set a seed\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.compat.v1.set_random_seed(1)\n",
    "\n",
    "#Importing the CIFAR10 Dataset\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "#tf.keras.utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch_size for Conveniece:\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset,https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "#Transforming the dataset into float32 arrays\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#Here I normalize the pixels by deviding it into the maximum value which is 255\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset Shape:\n",
      "X_train: (50000, 32, 32, 3)\n",
      "Y_train: (50000, 10)\n",
      "X_test:  (10000, 32, 32, 3)\n",
      "Y_test:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#They are RGB images in 3 channels!\n",
    "print('CIFAR-10 Dataset Shape:')\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(Y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('Y_test:  '  + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This are the lalbels 0,10 . The number 4 will be depicted as 4 is mapped to [0, 0, 0,0, 1, 0, 0, 0, 0, 0]\n",
    "classes = 10\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, classes)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We implement an image generator that allows us to augment the dataset.We are rotating the images 90 degrees to creat extra data.\n",
    "data_generator = ImageDataGenerator(rotation_range=90,\n",
    "                                    width_shift_range=0.1,\n",
    "                                    height_shift_range=0.1,\n",
    "                                    featurewise_center=True,\n",
    "#We basically just x=x-miu/standrd deviation. It it useful to keep values of the matrix close by and achieve vetter predictions when sampling. Is like removing outliers                              \n",
    "                                    featurewise_std_normalization=True,\n",
    "                                    horizontal_flip=True)\n",
    "#We just run the algorithm over the whole dataset\n",
    "data_generator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the test set\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i] = data_generator.standardize(X_test[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the network\n",
    "\n",
    "model=Sequential()\n",
    "#1. First Convolutional Layer. I padding=1 the borders of all images so they match.Input_shape=X_train.shape[1:]=(32,32,3)\n",
    "model.add(Convolution2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='same',\n",
    "                     input_shape=(32,32,3)))\n",
    "#2. Now I apply Batchnormalization function: It normalizes the outputs of the hidden layer for each\n",
    "#mini-batch (hence the name) in a way, which maintains its mean activation value close to 0,\n",
    "#and its standard deviation close to 1. Si it trains faster and has larger learning rates\n",
    "model.add(BatchNormalization())\n",
    "#3. We add an activation layer, in this case exponential linear unit (ELU), why?\n",
    "model.add(Activation('elu'))\n",
    "#4. We add a second activation layer no need to define the shape here\n",
    "model.add(Convolution2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='same'))\n",
    "#5. Add more normalizationhttps://towardsdatascience.com/batch-normalization-theory-and-how-to-use-it-with-tensorflow-1892ca0173ad\n",
    "model.add(BatchNormalization())\n",
    "#6. Another Activation Function\n",
    "model.add(Activation('elu'))\n",
    "#7. Do pooling to reduce dimensionality\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#8. We randomly drop a neuron while training the model with 20% probability to avoid overfitting.\n",
    "model.add(Dropout(0.2))\n",
    "#9. We apply a third Convolution Layer and fourth\n",
    "model.add(Convolution2D(filters=64,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(Convolution2D(filters=64,\n",
    "                     kernel_size=(3,3),\n",
    "                     padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "#10. Sigo Poninedo layers\n",
    "model.add(Convolution2D(128, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(Convolution2D(128, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#11. I flatten the input, create a normal neural network with 10 hidden layers, and an activatoion function\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we decide the loss function and optimization method\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 6, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 293,930\n",
      "Trainable params: 293,034\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rviccinig/anaconda3/envs/test-ai/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1000/1000 [==============================] - 128s 127ms/step - loss: 2.2078 - accuracy: 0.2794 - val_loss: 1.6099 - val_accuracy: 0.4390\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 127s 127ms/step - loss: 1.5986 - accuracy: 0.4239 - val_loss: 1.4917 - val_accuracy: 0.4969\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 128s 128ms/step - loss: 1.4185 - accuracy: 0.4907 - val_loss: 1.4322 - val_accuracy: 0.5131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5de1d32700>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=data_generator.flow(x=X_train,y=Y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=len(X_train) // batch_size,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
